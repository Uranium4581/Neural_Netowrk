{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc992e22",
   "metadata": {},
   "source": [
    "# ナッシュ均衡ベースのニューラルネットワークPruning\n",
    "\n",
    "このノートブックでは、ナッシュ均衡の概念を用いたニューラルネットワークのPruning（枝刈り）を実装します。\n",
    "\n",
    "## 概要\n",
    "- **参加度変数（Participation Variable）** `s ∈ [0,1]` を用いて各フィルタの重要度を学習\n",
    "- **L1正則化**と**L2正則化**を組み合わせてスパース性を促進\n",
    "- ResNet50V2アーキテクチャに適用してCIFAR-10データセットで評価\n",
    "\n",
    "## 環境設定\n",
    "TensorFlowのGPUメモリ管理を最適化するための環境変数を設定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07773a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ad1a8f",
   "metadata": {},
   "source": [
    "## ライブラリのインポート\n",
    "\n",
    "必要なライブラリをインポートします：\n",
    "- `tensorflow`: 深層学習フレームワーク\n",
    "- `numpy`, `matplotlib`: 数値計算と可視化\n",
    "- `MinMaxNorm`: 参加度変数の制約（0-1の範囲に制限）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8ccc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, mixed_precision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.constraints import MinMaxNorm\n",
    "from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2_as_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2f81b1",
   "metadata": {},
   "source": [
    "## GPU設定とMixed Precision\n",
    "\n",
    "- **GPUメモリ動的確保**: メモリスタベーションを防ぐため、必要に応じてメモリを割り当て\n",
    "- **Mixed Precision (混合精度)**: `float16`と`float32`を組み合わせて計算速度を向上させつつ、数値安定性を維持"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05577161",
   "metadata": {},
   "source": [
    "## FLOPs計算関数\n",
    "\n",
    "モデルの計算量（FLOPs: Floating Point Operations）を測定する関数です。\n",
    "\n",
    "FLOPsはモデルの推論速度の指標となり、Pruningの効果を評価する際に重要です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab5b579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPUメモリ動的確保（スタベーション対策）\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "        \n",
    "# 高速化設定 (Mixed Precision)\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "print(\"Mixed Precision Policy:\", mixed_precision.global_policy())\n",
    "\n",
    "print(\"TensorFlow Version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187ac674",
   "metadata": {},
   "source": [
    "## データセット準備\n",
    "\n",
    "CIFAR-10データセットを読み込み、前処理を行います：\n",
    "\n",
    "- **データ拡張**: リサイズ（32×32→64×64）、左右反転、明るさ・コントラスト調整\n",
    "- **正規化**: ピクセル値を0-1の範囲に正規化\n",
    "- **バッチ処理**: `tf.data`パイプラインを使用して効率的にデータを読み込み"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a554b64c",
   "metadata": {},
   "source": [
    "## ParticipatingConv2D層の実装\n",
    "\n",
    "### 参加度変数（Participation Variable）`s`について\n",
    "\n",
    "各フィルタに**参加度変数** `s ∈ [0,1]` を導入します：\n",
    "- `s = 1`: フィルタが完全に参加（通常の畳み込み）\n",
    "- `s = 0`: フィルタが非参加（実質的にpruning）\n",
    "- `0 < s < 1`: フィルタの出力がスケールされる\n",
    "\n",
    "### 正則化項\n",
    "\n",
    "損失関数に以下の2つのペナルティ項を追加します：\n",
    "\n",
    "#### L2正則化項\n",
    "$$\\beta \\sum_{i} \\|W_i\\|^2_2 \\cdot s_i^2$$\n",
    "\n",
    "- `β`: L2ペナルティ係数\n",
    "- `W_i`: フィルタ`i`の重み\n",
    "- `s_i`: フィルタ`i`の参加度\n",
    "- 参加度が大きいフィルタほど、重みのL2ノルムにペナルティがかかる\n",
    "\n",
    "#### L1正則化項（スパース性促進）\n",
    "$$\\gamma \\sum_{i} |s_i|$$\n",
    "\n",
    "- `γ`: L1ペナルティ係数\n",
    "- 参加度を直接スパース化することで、不要なフィルタを`0`に近づける\n",
    "\n",
    "### 損失関数の全体\n",
    "\n",
    "$$\\mathcal{L} = \\mathcal{L}_{task} + \\beta \\sum_{i} \\|W_i\\|^2_2 \\cdot s_i^2 + \\gamma \\sum_{i} |s_i|$$\n",
    "\n",
    "ここで、`L_task`はタスク損失（分類の場合はクロスエントロピー）です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b7d686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. FLOPs計算関数 \n",
    "\n",
    "def calculate_flops(model):\n",
    "    input_signature = [tf.TensorSpec(shape=(1,) + model.input_shape[1:], dtype=tf.float32)]\n",
    "    full_model = tf.function(lambda x: model(x))\n",
    "    concrete_func = full_model.get_concrete_function(input_signature)\n",
    "    frozen_func, _ = convert_variables_to_constants_v2_as_graph(concrete_func)\n",
    "    run_meta = tf.compat.v1.RunMetadata()\n",
    "    opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\n",
    "    flops = tf.compat.v1.profiler.profile(\n",
    "        graph=frozen_func.graph,\n",
    "        run_meta=run_meta, \n",
    "        cmd='op', \n",
    "        options=opts\n",
    "    )\n",
    "    return flops.total_float_ops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12929dd9",
   "metadata": {},
   "source": [
    "## 実装コード\n",
    "\n",
    "以下のセルで、ParticipatingConv2D層、ResNet50V2アーキテクチャ、Pruning統計コールバックを実装します。\n",
    "\n",
    "### 実装の構成\n",
    "1. **ParticipatingConv2D層**: 参加度変数付きの畳み込み層\n",
    "2. **ResNet50V2アーキテクチャ**: Bottleneck Blockとモデル構築関数\n",
    "3. **Pruning統計コールバック**: 学習中のPruning統計を記録"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57408c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. データセット準備 (CPU負荷分散版)\n",
    "BATCH_SIZE = 16\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# 正規化\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "# データ拡張関数 (tf.imageを使用)\n",
    "def augment(image, label):\n",
    "    image = tf.image.resize(image, [64, 64])  # 32x32 → 64x64\n",
    "    image = tf.image.random_flip_left_right(image)  # これも入れた方が良い\n",
    "    image = tf.image.random_brightness(image, max_delta=0.1)\n",
    "    image = tf.image.random_contrast(image, lower=0.9, upper=1.1)\n",
    "    return image, label\n",
    "\n",
    "# パイプライン構築\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)) \\\n",
    "    .shuffle(5000) \\\n",
    "    .map(augment, num_parallel_calls=tf.data.AUTOTUNE) \\\n",
    "    .batch(BATCH_SIZE) \\\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# テストデータもリサイズが必要\n",
    "def resize_only(image, label):\n",
    "    image = tf.image.resize(image, [64, 64])\n",
    "    return image, label\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)) \\\n",
    "    .map(resize_only, num_parallel_calls=tf.data.AUTOTUNE) \\\n",
    "    .batch(BATCH_SIZE) \\\n",
    "    .prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba534b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "\n",
    "# Participating Conv2D Layer (参加度変数付き畳み込み層)\n",
    "\n",
    "class ParticipatingConv2D(layers.Layer):\n",
    "    \"\"\"\n",
    "    ナッシュ均衡pruning用の畳み込み層\n",
    "    各フィルタに参加度変数 s ∈ [0,1] を持つ\n",
    "    \"\"\"\n",
    "    def __init__(self, filters, kernel_size, strides=1, padding='same', \n",
    "                 beta=0.05, gamma=0.05, use_bias=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size if isinstance(kernel_size, tuple) else (kernel_size, kernel_size)\n",
    "        self.strides = strides\n",
    "        self.padding = padding.upper()\n",
    "        self.use_bias = use_bias\n",
    "        \n",
    "        # ペナルティ係数\n",
    "        self.beta = beta   # L2 penalty\n",
    "        self.gamma = gamma # L1 penalty (sparsity)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # 畳み込みカーネル\n",
    "        self.kernel = self.add_weight(\n",
    "            name='kernel',\n",
    "            shape=(*self.kernel_size, input_shape[-1], self.filters),\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=True\n",
    "        )\n",
    "        \n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(\n",
    "                name='bias',\n",
    "                shape=(self.filters,),\n",
    "                initializer='zeros',\n",
    "                trainable=True\n",
    "            )\n",
    "        \n",
    "        # 参加度変数 (各フィルタに1つ)\n",
    "        self.participation = self.add_weight(\n",
    "        name='participation',\n",
    "        shape=(self.filters,),\n",
    "        initializer='ones',\n",
    "        trainable=True,\n",
    "        constraint=MinMaxNorm(min_value=0.0, max_value=1.0)\n",
    "        )\n",
    "\n",
    "        \n",
    "    def call(self, inputs, training=None):\n",
    "        # 畳み込み実行\n",
    "        if self.padding == 'SAME':\n",
    "            y = tf.nn.conv2d(inputs, self.kernel, strides=self.strides, padding='SAME')\n",
    "        else:\n",
    "            y = tf.nn.conv2d(inputs, self.kernel, strides=self.strides, padding='VALID')\n",
    "        \n",
    "        if self.use_bias:\n",
    "            y = tf.nn.bias_add(y, self.bias)\n",
    "        \n",
    "        # 参加度を掛ける (各フィルタの出力をスケール)\n",
    "        y = y * self.participation\n",
    "        \n",
    "        # 訓練時のみペナルティを追加\n",
    "        if training:\n",
    "            l2_per_filter = tf.reduce_sum(\n",
    "                tf.square(self.kernel), axis=[0, 1, 2]\n",
    "            )\n",
    "\n",
    "            l2_penalty = self.beta * tf.reduce_sum(\n",
    "                l2_per_filter * tf.square(self.participation)\n",
    "            )\n",
    "\n",
    "            l1_penalty = self.gamma * tf.reduce_sum(\n",
    "                tf.abs(self.participation)\n",
    "            )\n",
    "\n",
    "            # ★ ここが重要\n",
    "            self.add_loss(tf.cast(l2_penalty, tf.float32))\n",
    "            self.add_loss(tf.cast(l1_penalty, tf.float32))\n",
    "\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def get_active_filters(self, threshold=0.01):\n",
    "        \"\"\"参加度がthreshold以上のフィルタ数を返す\"\"\"\n",
    "        return tf.reduce_sum(tf.cast(self.participation > threshold, tf.int32)).numpy()\n",
    "    \n",
    "    def get_sparsity(self, threshold=0.01):\n",
    "        \"\"\"pruningされたフィルタの割合\"\"\"\n",
    "        active = self.get_active_filters(threshold)\n",
    "        return 1.0 - (active / self.filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9782627",
   "metadata": {},
   "source": [
    "### ResNet50V2アーキテクチャの構築\n",
    "\n",
    "`bottleneck_block_participating`関数と`build_resnet50_v2_participating`関数を定義します。\n",
    "\n",
    "これらの関数は、ParticipatingConv2D層を使用してResNet50V2アーキテクチャを構築します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54356ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet50V2 with Participating Convolutions\n",
    "\n",
    "def bottleneck_block_participating(x, filters, stride=1, beta=0.05, gamma=0.05):\n",
    "    \"\"\"\n",
    "    Participating Conv2D を使った Bottleneck Block\n",
    "    \"\"\"\n",
    "    shortcut = x\n",
    "    \n",
    "    # Pre-activation\n",
    "    pre_act = layers.BatchNormalization()(x)\n",
    "    pre_act = layers.Activation(\"relu\")(pre_act)\n",
    "    \n",
    "    # Shortcut調整\n",
    "    if stride > 1 or x.shape[-1] != filters * 4:\n",
    "        # ショートカットは通常のConvでOK\n",
    "        shortcut = layers.Conv2D(filters * 4, 1, strides=stride, use_bias=False)(pre_act)\n",
    "    \n",
    "    # Main path with participating convolutions\n",
    "    # 1x1 Conv (圧縮)\n",
    "    m = ParticipatingConv2D(filters, 1, beta=beta, gamma=gamma)(pre_act)\n",
    "    m = layers.BatchNormalization()(m)\n",
    "    m = layers.Activation(\"relu\")(m)\n",
    "    \n",
    "    # 3x3 Conv (特徴抽出) - ここが重要\n",
    "    m = layers.ZeroPadding2D(padding=1)(m)\n",
    "    m = ParticipatingConv2D(filters, 3, strides=stride, padding='valid', \n",
    "                           beta=beta, gamma=gamma)(m)\n",
    "    m = layers.BatchNormalization()(m)\n",
    "    m = layers.Activation(\"relu\")(m)\n",
    "    \n",
    "    # 1x1 Conv (復元)\n",
    "    m = ParticipatingConv2D(filters * 4, 1, beta=beta, gamma=gamma)(m)\n",
    "    \n",
    "    return layers.Add()([shortcut, m])\n",
    "\n",
    "\n",
    "def build_resnet50_v2_participating(input_shape=(48, 48, 3), classes=10, \n",
    "                                   beta=0.05, gamma=0.05):\n",
    "    \"\"\"\n",
    "    Nash equilibrium pruning用のResNet50V2\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(input_shape)\n",
    "    \n",
    "    # Stem (最初は通常のConv)\n",
    "    x = layers.Conv2D(64, 3, strides=1, padding=\"same\", use_bias=False)(inputs)\n",
    "    \n",
    "    # Stage 1\n",
    "    for _ in range(3):\n",
    "        x = bottleneck_block_participating(x, 64, stride=1, beta=beta, gamma=gamma)\n",
    "    \n",
    "    # Stage 2\n",
    "    x = bottleneck_block_participating(x, 128, stride=2, beta=beta, gamma=gamma)\n",
    "    for _ in range(3):\n",
    "        x = bottleneck_block_participating(x, 128, stride=1, beta=beta, gamma=gamma)\n",
    "    \n",
    "    # Stage 3\n",
    "    x = bottleneck_block_participating(x, 256, stride=2, beta=beta, gamma=gamma)\n",
    "    for _ in range(5):\n",
    "        x = bottleneck_block_participating(x, 256, stride=1, beta=beta, gamma=gamma)\n",
    "    \n",
    "    # Stage 4\n",
    "    x = bottleneck_block_participating(x, 512, stride=2, beta=beta, gamma=gamma)\n",
    "    for _ in range(2):\n",
    "        x = bottleneck_block_participating(x, 512, stride=1, beta=beta, gamma=gamma)\n",
    "    \n",
    "    # Head\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    outputs = layers.Dense(classes, activation=\"softmax\", dtype='float32')(x)\n",
    "    \n",
    "    return models.Model(inputs, outputs, name=\"ResNet50V2_Nash_Pruning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def6a9c4",
   "metadata": {},
   "source": [
    "### Pruning統計コールバックの実装\n",
    "\n",
    "`PruningStatsCallback`クラスを定義します。このコールバックは各エポック後に参加度統計を記録し、Pruningの進行状況を監視します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a4a12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pruning Statistics Callback\n",
    "\n",
    "class PruningStatsCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    各エポック後に参加度統計を記録するコールバック\n",
    "    \"\"\"\n",
    "    def __init__(self, threshold=0.01):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "        self.history = {\n",
    "            'active_filters': [],\n",
    "            'sparsity': [],\n",
    "            'mean_participation': []\n",
    "        }\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        total_filters = 0\n",
    "        active_filters = 0\n",
    "        participation_sum = 0.0\n",
    "        \n",
    "        for layer in self.model.layers:\n",
    "            if isinstance(layer, ParticipatingConv2D):\n",
    "                total_filters += layer.filters\n",
    "                active_filters += layer.get_active_filters(self.threshold)\n",
    "                participation_sum += tf.reduce_sum(layer.participation).numpy()\n",
    "        \n",
    "        sparsity = 1.0 - (active_filters / total_filters) if total_filters > 0 else 0.0\n",
    "        mean_participation = participation_sum / total_filters if total_filters > 0 else 0.0\n",
    "        \n",
    "        self.history['active_filters'].append(active_filters)\n",
    "        self.history['sparsity'].append(sparsity)\n",
    "        self.history['mean_participation'].append(mean_participation)\n",
    "        \n",
    "        print(f\"\\n[Pruning Stats] Active: {active_filters}/{total_filters} \"\n",
    "              f\"({(1-sparsity)*100:.1f}%), Mean s: {mean_participation:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b957552",
   "metadata": {},
   "source": [
    "## モデル構築とFLOPs測定\n",
    "\n",
    "Baselineモデルを構築し、計算量（FLOPs）とパラメータ数を測定します。\n",
    "\n",
    "この時点では`beta`と`gamma`を小さく設定（`1e-5`）して、ほぼ通常のモデルとして動作させます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b188b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. モデル構築とFLOPs測定\n",
    "model = build_resnet50_v2_participating(\n",
    "    input_shape=(64, 64, 3),\n",
    "    classes=10,\n",
    "    beta=1e-5,\n",
    "    gamma=1e-5\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"【Baseline (Normal) モデル計算量】\")\n",
    "print(\"=\"*40)\n",
    "try:\n",
    "    flops_val = calculate_flops(model)\n",
    "    params = model.count_params()\n",
    "    print(f\"パラメータ数: {params:,}\")\n",
    "    print(f\"FLOPs: {flops_val / 10**9:.4f} G (ギガ)\")\n",
    "except Exception as e:\n",
    "    print(f\"FLOPs計算エラー: {e}\")\n",
    "print(\"=\"*40 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4404bf49",
   "metadata": {},
   "source": [
    "## モデルの学習\n",
    "\n",
    "モデルをコンパイルし、訓練を実行します。\n",
    "\n",
    "### 設定\n",
    "- **オプティマイザ**: Adam\n",
    "- **損失関数**: Sparse Categorical Crossentropy\n",
    "- **コールバック**:\n",
    "  - `PruningStatsCallback`: 各エポック後にPruning統計を記録\n",
    "  - `EarlyStopping`: 検証損失が改善しない場合に訓練を早期終了（patience=15）\n",
    "\n",
    "### 注意\n",
    "`add_loss()`メソッドにより、L1/L2正則化項が自動的に損失関数に追加されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14a33a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 学習実行\n",
    "pruning_stats = PruningStatsCallback(threshold=0.01)\n",
    "# 1. compile\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# 2. callbacks\n",
    "pruning_stats = PruningStatsCallback(threshold=0.01)\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=15,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# 3. fit（1回だけ）\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=test_ds,\n",
    "    epochs=50,\n",
    "    callbacks=[early_stop, pruning_stats]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a45c51",
   "metadata": {},
   "source": [
    "## 学習結果の確認\n",
    "\n",
    "学習履歴から基本的な統計情報を表示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd66d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 結果の可視化（個別保存版）\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs_range = range(len(acc))\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"学習終了: 全{len(acc)}エポック (0-{len(acc)-1})\")\n",
    "print(f\"最終 Val Accuracy: {val_acc[-1]:.4f}\")\n",
    "print(f\"最終 Val Loss: {val_loss[-1]:.4f}\")\n",
    "print(f\"{'='*50}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875c2d6a",
   "metadata": {},
   "source": [
    "## Accuracyの可視化\n",
    "\n",
    "訓練データと検証データのAccuracyを時系列でプロットします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a725d226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracyグラフ\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(f'Baseline Accuracy (Final: {val_acc[-1]:.4f})')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(0, len(acc))\n",
    "plt.xticks(range(0, len(acc), 2))\n",
    "plt.tight_layout()\n",
    "plt.savefig('baseline_accuracy.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528b30f6",
   "metadata": {},
   "source": [
    "## Lossの可視化\n",
    "\n",
    "訓練データと検証データのLossを時系列でプロットします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e88ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lossグラフ\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f'Baseline Loss (Final: {val_loss[-1]:.4f})')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(0, len(acc))\n",
    "plt.xticks(range(0, len(acc), 2))\n",
    "plt.tight_layout()\n",
    "plt.savefig('baseline_loss.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff45b40",
   "metadata": {},
   "source": [
    "## モデルの保存\n",
    "\n",
    "学習済みモデルと可視化結果を保存します。\n",
    "\n",
    "- **モデルファイル**: `resnet50v2_baseline.h5`\n",
    "- **グラフ**: `baseline_accuracy.png`, `baseline_loss.png`\n",
    "\n",
    "これらのファイルは後の比較実験で使用されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f63893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存（後の比較用）\n",
    "model.save(\"resnet50v2_baseline.h5\")\n",
    "print(\"\\n保存完了:\")\n",
    "print(\"  - baseline_accuracy.png\")\n",
    "print(\"  - baseline_loss.png\")\n",
    "print(\"  - resnet50v2_baseline.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
